{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMn/PfUFO4Lk1WfQo4pbSNF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Data reading"],"metadata":{"id":"EjIFN7PPYyVT"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cwjp9GV4uxQw","executionInfo":{"status":"ok","timestamp":1718174303923,"user_tz":-480,"elapsed":13,"user":{"displayName":"Palash","userId":"17952405708644180763"}},"outputId":"6e88d35d-0663-43cd-8690-4ed432961023"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-06-12 06:38:23--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1115394 (1.1M) [text/plain]\n","Saving to: ‘input.txt’\n","\n","\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n","\n","2024-06-12 06:38:23 (18.4 MB/s) - ‘input.txt’ saved [1115394/1115394]\n","\n"]}],"source":["!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"]},{"cell_type":"code","source":["with open('input.txt', 'r', encoding='utf-8') as f:\n","    text = f.read()"],"metadata":{"id":"dqRXawC0u_-x","executionInfo":{"status":"ok","timestamp":1718174303924,"user_tz":-480,"elapsed":9,"user":{"displayName":"Palash","userId":"17952405708644180763"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["print(len(text), len(text.split()), len(set(text)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B1zVOVnvvE7T","executionInfo":{"status":"ok","timestamp":1718174303924,"user_tz":-480,"elapsed":8,"user":{"displayName":"Palash","userId":"17952405708644180763"}},"outputId":"5aeea597-ea3f-42a2-afc5-961d938a4d40"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["1115394 202651 65\n"]}]},{"cell_type":"markdown","source":["# Data processing"],"metadata":{"id":"2dZjwcC8Y-vk"}},{"cell_type":"code","source":["chars = sorted(set(text))\n","vocab_size = len(chars)\n","print(f\"{vocab_size=}\\n<S>{'|'.join(chars)}<E>\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JnaUgM6HvHHB","executionInfo":{"status":"ok","timestamp":1718174303924,"user_tz":-480,"elapsed":6,"user":{"displayName":"Palash","userId":"17952405708644180763"}},"outputId":"b1ed75a9-2332-48ae-c850-f476f32918d5"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["vocab_size=65\n","<S>\n","| |!|$|&|'|,|-|.|3|:|;|?|A|B|C|D|E|F|G|H|I|J|K|L|M|N|O|P|Q|R|S|T|U|V|W|X|Y|Z|a|b|c|d|e|f|g|h|i|j|k|l|m|n|o|p|q|r|s|t|u|v|w|x|y|z<E>\n"]}]},{"cell_type":"code","source":["# creating vocabulary and mapping and reverse mapping\n","itos = {i:c for i, c in enumerate(chars)}\n","stoi = {c:i for i, c in itos.items()}\n","encode = lambda s : [stoi[c] for c in s]\n","decode = lambda e : ''.join([itos[i] for i in e])\n","\n","sample_text = \"what's up\"\n","print(encode(sample_text))\n","print(decode(encode(sample_text)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qZYvlck2vTmv","executionInfo":{"status":"ok","timestamp":1718174303924,"user_tz":-480,"elapsed":5,"user":{"displayName":"Palash","userId":"17952405708644180763"}},"outputId":"406e2cbf-0466-4ea8-8b91-317a15dd10aa"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[61, 46, 39, 58, 5, 57, 1, 59, 54]\n","what's up\n"]}]},{"cell_type":"markdown","source":["## few notes about tokenizer usage"],"metadata":{"id":"GnMcPnz5_hkE"}},{"cell_type":"markdown","source":["Try other encodings as well.\n","e.g.\n","* SentencePiece\n","* Also BPE's fast implementation is tiktoken used in GPT models created by OpenAI\n","* OpenAI's tokenizer demo - https://platform.openai.com/tokenizer\n","* Third party tokenizer demo - https://tiktokenizer.vercel.app/\n","* tiktoken repo - https://github.com/openai/tiktoken\n","* sample tiktoken usage cookbook - https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n","\n","\n","---\n","\n","\n","**ToDo** - Learn about BPE tokenizer process in depth."],"metadata":{"id":"nWqRgSsS6biK"}},{"cell_type":"code","source":["#!pip install tiktoken\n","import tiktoken\n","enc = tiktoken.get_encoding('gpt2')\n","print(enc.n_vocab)\n","print(enc.encode(sample_text))\n","print(enc.decode(enc.encode(sample_text)))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TOcDq4Er46kE","executionInfo":{"status":"ok","timestamp":1718174324804,"user_tz":-480,"elapsed":11746,"user":{"displayName":"Palash","userId":"17952405708644180763"}},"outputId":"8532ce94-cef9-426f-a105-b0192e26b43f"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tiktoken\n","  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n","Installing collected packages: tiktoken\n","Successfully installed tiktoken-0.7.0\n","50257\n","[10919, 338, 510]\n","what's up\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"sUgylD5JYxNg"}},{"cell_type":"markdown","source":["## data processing continued (with character encoding)"],"metadata":{"id":"rmHwT5_s_yR3"}},{"cell_type":"code","source":["import torch\n","#encode text into token\n","data = torch.tensor(encode(text), dtype=torch.long)\n","print(f\"{data.shape=}, {data.dtype=}\")\n","print(data[:100])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N-823NrI61ZN","executionInfo":{"status":"ok","timestamp":1718174334839,"user_tz":-480,"elapsed":5832,"user":{"displayName":"Palash","userId":"17952405708644180763"}},"outputId":"05df2522-baa3-49c4-90df-88c4914fa20a"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["data.shape=torch.Size([1115394]), data.dtype=torch.int64\n","tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n","        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n","         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n","        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n","         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n","        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"]}]},{"cell_type":"code","source":["# train, val split\n","split = int(0.9*len(data))\n","train_data = data[:split]\n","val_data = data[split:]\n","print(f\"{train_data.shape=}, {val_data.shape=}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b_-0L4pwBB01","executionInfo":{"status":"ok","timestamp":1718174341806,"user_tz":-480,"elapsed":1030,"user":{"displayName":"Palash","userId":"17952405708644180763"}},"outputId":"7c9777d7-0a43-4183-b335-f1a83f604bb5"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["train_data.shape=torch.Size([1003854]), val_data.shape=torch.Size([111540])\n"]}]},{"cell_type":"code","source":["# get batches of data in block_size\n","\n","block_size = 8 # also called as context length\n","batch_size = 4 # for parallel processing\n","\n","torch.manual_seed(1337)\n","def get_batch(split):\n","  data = train_data if split=='train' else val_data\n","  batch_ixs = torch.randint(0, len(data) - block_size, (batch_size,))\n","  x = torch.stack([data[ix : ix + block_size] for ix in batch_ixs])\n","  y = torch.stack([data[ix + 1: ix + 1 + block_size] for ix in batch_ixs])\n","  '''\n","  if x indices are  [3,4,5,6,7,8,9, 10]\n","  y indices will be [4,5,6,7,8,9,10,11]\n","\n","  torch.stack will just stack in one extra dimension in front\n","  '''\n","  return x, y\n","\n","xb, yb = get_batch('train')\n","print(f\"{xb.shape=}, {yb.shape=}\")\n","print(xb)\n","print(yb)\n","\n","for bt in range(batch_size):\n","  for bl in range(block_size):\n","    context = xb[bt, :bl+1]\n","    output  = yb[bt, bl]\n","    print(f\"{decode(context.numpy())} --> {decode([output.item()])}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gqyZz2JiA-Qh","executionInfo":{"status":"ok","timestamp":1718174344267,"user_tz":-480,"elapsed":623,"user":{"displayName":"Palash","userId":"17952405708644180763"}},"outputId":"bd2a25e5-d668-4d74-dbac-60ef9e3b086b"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["xb.shape=torch.Size([4, 8]), yb.shape=torch.Size([4, 8])\n","tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n","        [44, 53, 56,  1, 58, 46, 39, 58],\n","        [52, 58,  1, 58, 46, 39, 58,  1],\n","        [25, 17, 27, 10,  0, 21,  1, 54]])\n","tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n","        [53, 56,  1, 58, 46, 39, 58,  1],\n","        [58,  1, 58, 46, 39, 58,  1, 46],\n","        [17, 27, 10,  0, 21,  1, 54, 39]])\n","L --> e\n","Le --> t\n","Let --> '\n","Let' --> s\n","Let's -->  \n","Let's  --> h\n","Let's h --> e\n","Let's he --> a\n","f --> o\n","fo --> r\n","for -->  \n","for  --> t\n","for t --> h\n","for th --> a\n","for tha --> t\n","for that -->  \n","n --> t\n","nt -->  \n","nt  --> t\n","nt t --> h\n","nt th --> a\n","nt tha --> t\n","nt that -->  \n","nt that  --> h\n","M --> E\n","ME --> O\n","MEO --> :\n","MEO: --> \n","\n","MEO:\n"," --> I\n","MEO:\n","I -->  \n","MEO:\n","I  --> p\n","MEO:\n","I p --> a\n"]}]},{"cell_type":"markdown","source":["# lets start with simple bigram model"],"metadata":{"id":"1kcJfAchbKmt"}},{"cell_type":"markdown","source":["learn more about torch libraries - specifically start with nn.Module"],"metadata":{"id":"q-byeANNbuzG"}},{"cell_type":"code","source":["xb.shape, yb.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Iu0kE2g4fjFx","executionInfo":{"status":"ok","timestamp":1718174398226,"user_tz":-480,"elapsed":412,"user":{"displayName":"Palash","userId":"17952405708644180763"}},"outputId":"cb30a3c1-efca-4b2e-e720-bf577808a78e"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([4, 8]), torch.Size([4, 8]))"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class BigramLanguageModel(nn.Module):\n","  def __init__(self, vocab_size):\n","    super().__init__()\n","    self.token_embedding = nn.Embedding(vocab_size, vocab_size)\n","\n","  def forward(self, x, targets=None):\n","    logits = self.token_embedding(x) # converts shape(B, T) to shape(B, T, C) | B-batch, T-?, C-Channel(Embedding_size)\n","\n","    loss = None\n","    if targets is not None:\n","      B,T,C = logits.shape\n","      # read documentation of cross_entropy for reason of this reshaping -\n","      # https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html#torch.nn.functional.cross_entropy\n","      logits = logits.view(B*T, C)\n","      targets = targets.view(B*T) # or just -1\n","      loss = F.cross_entropy(logits, targets)\n","\n","    return logits, loss\n","\n","  def generate(self, idx, max_new_tokens):\n","    for _ in range(max_new_tokens):\n","      logits, _ = self(idx)             # forward pass to get logits, loss is not needed\n","      logits = logits[:, -1, :]         # logits will be of shape B,T,C but we only care about last character i.e. last T\n","      probs = F.softmax(logits, dim=1)  # prob using softmax along the channel dimension\n","      next_idx = torch.multinomial(probs, 1)\n","      # that is, we are training with (4,8) -> (4,8) shape. But feeding(1, +inf) shape to get (1, +inf) out and then just consume only last T.\n","      # validate this fact\n","      #print(f\"debug | {decode(idx[0].numpy())} --> {decode([next_idx[0].item()])}\") # 0-index to take from first batch\n","      idx = torch.cat([idx, next_idx], dim=1)\n","    return idx"],"metadata":{"id":"hlhVMLYOUWWI","executionInfo":{"status":"ok","timestamp":1718176449988,"user_tz":-480,"elapsed":378,"user":{"displayName":"Palash","userId":"17952405708644180763"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["model = BigramLanguageModel(vocab_size)\n","logits, loss = model(xb, yb)\n","print(f\"{logits=}, {loss=}\")"],"metadata":{"id":"4m1dE7GHb3Oj","executionInfo":{"status":"ok","timestamp":1718176455112,"user_tz":-480,"elapsed":349,"user":{"displayName":"Palash","userId":"17952405708644180763"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2c254582-5b3d-4ce7-b10b-fdc0a6be8aa0"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["logits=tensor([[ 0.0926,  0.5827, -0.6639,  ...,  0.8576,  0.5767, -0.1432],\n","        [-0.0906, -1.1605,  1.2265,  ..., -0.3734, -0.5206,  0.7613],\n","        [-0.8223, -0.5409,  0.1870,  ..., -1.3379,  0.0693,  2.2249],\n","        ...,\n","        [-0.2609,  0.7606, -0.9424,  ..., -1.5740,  0.8685, -0.8768],\n","        [ 0.6698,  0.2120,  0.5794,  ...,  0.8288,  2.3338,  0.9831],\n","        [-0.7894, -1.1862,  0.3646,  ..., -1.4089,  1.4106, -0.2585]],\n","       grad_fn=<ViewBackward0>), loss=tensor(4.6343, grad_fn=<NllLossBackward0>)\n"]}]},{"cell_type":"code","source":["# we are starting with a batch and 1 context length to start with\n","\n","def generate_with_no_context(max_new_tokens=20):\n","  #model.generate(torch.tensor([stoi[' ']]*block_size).view(1, -1), max_new_tokens=10)\n","  inp = torch.zeros((1,1), dtype=torch.long) # zero is newline char\n","  out = model.generate(inp, max_new_tokens=max_new_tokens)\n","  print(decode(out[0].tolist())) # 0-index to pick batch index\n","\n","generate_with_no_context()"],"metadata":{"id":"0CqI3oowKz0F","executionInfo":{"status":"ok","timestamp":1718176782189,"user_tz":-480,"elapsed":412,"user":{"displayName":"Palash","userId":"17952405708644180763"}}},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":["## lets try to create and use an optimizer to train model\n","* read API and docs here - https://pytorch.org/docs/stable/optim.html\n","* **ToDo** - learn about different optimizers, esp Adam(W)"],"metadata":{"id":"BlPu6vwVzKR5"}},{"cell_type":"code","source":["optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"],"metadata":{"id":"sYgC1rgHNU19","executionInfo":{"status":"ok","timestamp":1718176509871,"user_tz":-480,"elapsed":2,"user":{"displayName":"Palash","userId":"17952405708644180763"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["batch_size = 32\n","max_steps = 100000\n","for i in range(max_steps):\n","  xb, yb = get_batch('train')\n","  logits, loss = model(xb, yb)\n","  optimizer.zero_grad(set_to_none=True) # this set_to_none is for performance optimization. but can lead to error if try to access grad\n","  loss.backward()\n","  optimizer.step()\n","  if i%10000==0:\n","    print(loss.item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-wct_1L60jVg","executionInfo":{"status":"ok","timestamp":1718176753425,"user_tz":-480,"elapsed":241878,"user":{"displayName":"Palash","userId":"17952405708644180763"}},"outputId":"b439d63a-f38b-40ea-85fb-5a0663a1cfbd"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["4.727841854095459\n","2.5576331615448\n","2.4996368885040283\n","2.397784471511841\n","2.4928104877471924\n","2.463300943374634\n","2.507370948791504\n","2.4625484943389893\n","2.5009632110595703\n","2.4473514556884766\n"]}]},{"cell_type":"code","source":["generate_with_no_context(max_new_tokens=200)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yllT9DxVL-WM","executionInfo":{"status":"ok","timestamp":1718176797861,"user_tz":-480,"elapsed":5,"user":{"displayName":"Palash","userId":"17952405708644180763"}},"outputId":"ffc80d75-689c-41a0-daa0-1265dd048afd"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Wham'de l:\n","MNUTLAs?\n","PA k My the thofim,\n","ARKis, her stoofous w weryon IO:\n","apr ce f ar sor n!\n","So,\n","Towinccan, hen.\n","bu s st LYCLOry aithot: twoforoove nd IO:\n","Youl wngd ha th a ou\n","Co heishimongor we poono\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"teLqxOX4TKkp"},"execution_count":null,"outputs":[]}]}