{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPaUZuMXnDsKUNaY32tURB6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Data reading"],"metadata":{"id":"EjIFN7PPYyVT"}},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cwjp9GV4uxQw","executionInfo":{"status":"ok","timestamp":1718199460934,"user_tz":-480,"elapsed":895,"user":{"displayName":"Palash","userId":"17952405708644180763"}},"outputId":"8efe780a-c0ad-456a-ef7f-8dc5443f5dea"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-06-12 13:37:39--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1115394 (1.1M) [text/plain]\n","Saving to: ‘input.txt’\n","\n","input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n","\n","2024-06-12 13:37:40 (16.9 MB/s) - ‘input.txt’ saved [1115394/1115394]\n","\n"]}],"source":["!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"]},{"cell_type":"code","source":["with open('input.txt', 'r', encoding='utf-8') as f:\n","  text = f.read()"],"metadata":{"id":"dqRXawC0u_-x","executionInfo":{"status":"ok","timestamp":1718199460934,"user_tz":-480,"elapsed":14,"user":{"displayName":"Palash","userId":"17952405708644180763"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["print(len(text), len(text.split()), len(set(text)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B1zVOVnvvE7T","executionInfo":{"status":"ok","timestamp":1718199460935,"user_tz":-480,"elapsed":12,"user":{"displayName":"Palash","userId":"17952405708644180763"}},"outputId":"704a6cfd-f2a5-43c8-ec39-87999a9b23cd"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["1115394 202651 65\n"]}]},{"cell_type":"markdown","source":["# Data processing"],"metadata":{"id":"2dZjwcC8Y-vk"}},{"cell_type":"code","source":["chars = sorted(set(text))\n","vocab_size = len(chars)\n","print(f\"{vocab_size=}\\n<S>{'|'.join(chars)}<E>\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JnaUgM6HvHHB","executionInfo":{"status":"ok","timestamp":1718199460935,"user_tz":-480,"elapsed":8,"user":{"displayName":"Palash","userId":"17952405708644180763"}},"outputId":"3e058743-cfe6-4a20-9aaf-9db6ee1c5eb2"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["vocab_size=65\n","<S>\n","| |!|$|&|'|,|-|.|3|:|;|?|A|B|C|D|E|F|G|H|I|J|K|L|M|N|O|P|Q|R|S|T|U|V|W|X|Y|Z|a|b|c|d|e|f|g|h|i|j|k|l|m|n|o|p|q|r|s|t|u|v|w|x|y|z<E>\n"]}]},{"cell_type":"code","source":["# creating vocabulary and mapping and reverse mapping\n","itos = {i:c for i, c in enumerate(chars)}\n","stoi = {c:i for i, c in itos.items()}\n","encode = lambda s : [stoi[c] for c in s]\n","decode = lambda e : ''.join([itos[i] for i in e])\n","\n","sample_text = \"what's up\"\n","print(encode(sample_text))\n","print(decode(encode(sample_text)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qZYvlck2vTmv","executionInfo":{"status":"ok","timestamp":1718199460935,"user_tz":-480,"elapsed":5,"user":{"displayName":"Palash","userId":"17952405708644180763"}},"outputId":"0b6057fd-c041-4005-876f-d6a93a88633b"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["[61, 46, 39, 58, 5, 57, 1, 59, 54]\n","what's up\n"]}]},{"cell_type":"markdown","source":["## few notes about tokenizer usage"],"metadata":{"id":"GnMcPnz5_hkE"}},{"cell_type":"markdown","source":["Try other encodings as well.\n","e.g.\n","* SentencePiece\n","* Also BPE's fast implementation is tiktoken used in GPT models created by OpenAI\n","* OpenAI's tokenizer demo - https://platform.openai.com/tokenizer\n","* Third party tokenizer demo - https://tiktokenizer.vercel.app/\n","* tiktoken repo - https://github.com/openai/tiktoken\n","* sample tiktoken usage cookbook - https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n","\n","\n","---\n","\n","\n","**ToDo** - Learn about BPE tokenizer process in depth."],"metadata":{"id":"nWqRgSsS6biK"}},{"cell_type":"code","source":["#!pip install tiktoken\n","import tiktoken\n","enc = tiktoken.get_encoding('gpt2')\n","print(enc.n_vocab)\n","print(enc.encode(sample_text))\n","print(enc.decode(enc.encode(sample_text)))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TOcDq4Er46kE","executionInfo":{"status":"ok","timestamp":1718199480877,"user_tz":-480,"elapsed":13051,"user":{"displayName":"Palash","userId":"17952405708644180763"}},"outputId":"1c78f153-0235-4acb-b75e-912ab14d446d"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tiktoken\n","  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n","Installing collected packages: tiktoken\n","Successfully installed tiktoken-0.7.0\n","50257\n","[10919, 338, 510]\n","what's up\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"sUgylD5JYxNg"}},{"cell_type":"markdown","source":["## data processing continued (with character encoding)"],"metadata":{"id":"rmHwT5_s_yR3"}},{"cell_type":"code","source":["import torch\n","#encode text into token\n","data = torch.tensor(encode(text), dtype=torch.long)\n","print(f\"{data.shape=}, {data.dtype=}\")\n","print(data[:100])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N-823NrI61ZN","executionInfo":{"status":"ok","timestamp":1718199480878,"user_tz":-480,"elapsed":7,"user":{"displayName":"Palash","userId":"17952405708644180763"}},"outputId":"595327a3-47bb-4d10-93dc-fe493b0901a8"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["data.shape=torch.Size([1115394]), data.dtype=torch.int64\n","tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n","        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n","         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n","        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n","         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n","        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"]}]},{"cell_type":"code","source":["# train, val split\n","split = int(0.9*len(data))\n","train_data = data[:split]\n","val_data = data[split:]\n","print(f\"{train_data.shape=}, {val_data.shape=}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b_-0L4pwBB01","executionInfo":{"status":"ok","timestamp":1718178253895,"user_tz":-480,"elapsed":3,"user":{"displayName":"Palash","userId":"17952405708644180763"}},"outputId":"e82e4f60-8158-49dd-c9d9-556ff80e6035"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["train_data.shape=torch.Size([1003854]), val_data.shape=torch.Size([111540])\n"]}]},{"cell_type":"code","source":["# get batches of data in block_size\n","\n","block_size = 8 # also called as context length\n","batch_size = 4 # for parallel processing\n","\n","torch.manual_seed(1337)\n","def get_batch(split):\n","  data = train_data if split=='train' else val_data\n","  batch_ixs = torch.randint(0, len(data) - block_size, (batch_size,))\n","  x = torch.stack([data[ix : ix + block_size] for ix in batch_ixs])\n","  y = torch.stack([data[ix + 1: ix + 1 + block_size] for ix in batch_ixs])\n","  '''\n","  if x indices are  [3,4,5,6,7,8,9, 10]\n","  y indices will be [4,5,6,7,8,9,10,11]\n","\n","  torch.stack will just stack in one extra dimension in front\n","  '''\n","  return x, y\n","\n","xb, yb = get_batch('train')\n","print(f\"{xb.shape=}, {yb.shape=}\")\n","print(xb)\n","print(yb)\n","\n","for bt in range(batch_size):\n","  for bl in range(block_size):\n","    context = xb[bt, :bl+1]\n","    output  = yb[bt, bl]\n","    print(f\"{decode(context.numpy())} --> {decode([output.item()])}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gqyZz2JiA-Qh","executionInfo":{"status":"ok","timestamp":1718174344267,"user_tz":-480,"elapsed":623,"user":{"displayName":"Palash","userId":"17952405708644180763"}},"outputId":"bd2a25e5-d668-4d74-dbac-60ef9e3b086b"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["xb.shape=torch.Size([4, 8]), yb.shape=torch.Size([4, 8])\n","tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n","        [44, 53, 56,  1, 58, 46, 39, 58],\n","        [52, 58,  1, 58, 46, 39, 58,  1],\n","        [25, 17, 27, 10,  0, 21,  1, 54]])\n","tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n","        [53, 56,  1, 58, 46, 39, 58,  1],\n","        [58,  1, 58, 46, 39, 58,  1, 46],\n","        [17, 27, 10,  0, 21,  1, 54, 39]])\n","L --> e\n","Le --> t\n","Let --> '\n","Let' --> s\n","Let's -->  \n","Let's  --> h\n","Let's h --> e\n","Let's he --> a\n","f --> o\n","fo --> r\n","for -->  \n","for  --> t\n","for t --> h\n","for th --> a\n","for tha --> t\n","for that -->  \n","n --> t\n","nt -->  \n","nt  --> t\n","nt t --> h\n","nt th --> a\n","nt tha --> t\n","nt that -->  \n","nt that  --> h\n","M --> E\n","ME --> O\n","MEO --> :\n","MEO: --> \n","\n","MEO:\n"," --> I\n","MEO:\n","I -->  \n","MEO:\n","I  --> p\n","MEO:\n","I p --> a\n"]}]},{"cell_type":"markdown","source":["# lets start with simple bigram model"],"metadata":{"id":"1kcJfAchbKmt"}},{"cell_type":"markdown","source":["learn more about torch libraries - specifically start with nn.Module"],"metadata":{"id":"q-byeANNbuzG"}},{"cell_type":"code","source":["xb.shape, yb.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Iu0kE2g4fjFx","executionInfo":{"status":"ok","timestamp":1718174398226,"user_tz":-480,"elapsed":412,"user":{"displayName":"Palash","userId":"17952405708644180763"}},"outputId":"cb30a3c1-efca-4b2e-e720-bf577808a78e"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([4, 8]), torch.Size([4, 8]))"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class BigramLanguageModel(nn.Module):\n","  def __init__(self, vocab_size):\n","    super().__init__()\n","    self.token_embedding = nn.Embedding(vocab_size, vocab_size)\n","\n","  def forward(self, x, targets=None):\n","    logits = self.token_embedding(x) # converts shape(B, T) to shape(B, T, C) | B-batch, T-Time, C-Channel(Embedding_size)\n","\n","    loss = None\n","    if targets is not None:\n","      B,T,C = logits.shape\n","      # read documentation of cross_entropy for reason of this reshaping -\n","      # https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html#torch.nn.functional.cross_entropy\n","      logits = logits.view(B*T, C)\n","      targets = targets.view(B*T) # or just -1\n","      loss = F.cross_entropy(logits, targets)\n","\n","    return logits, loss\n","\n","  def generate(self, idx, max_new_tokens):\n","    for _ in range(max_new_tokens):\n","      logits, _ = self(idx)             # forward pass to get logits, loss is not needed\n","      logits = logits[:, -1, :]         # logits will be of shape B,T,C but we only care about last character i.e. last T\n","      probs = F.softmax(logits, dim=1)  # prob using softmax along the channel dimension\n","      next_idx = torch.multinomial(probs, 1)\n","      # that is, we are training with (4,8) -> (4,8) shape. But feeding(1, +inf) shape to get (1, +inf) out and then just consume only last T.\n","      # validate this fact\n","      #print(f\"debug | {decode(idx[0].numpy())} --> {decode([next_idx[0].item()])}\") # 0-index to take from first batch\n","      idx = torch.cat([idx, next_idx], dim=1)\n","    return idx"],"metadata":{"id":"hlhVMLYOUWWI","executionInfo":{"status":"ok","timestamp":1718199439817,"user_tz":-480,"elapsed":5856,"user":{"displayName":"Palash","userId":"17952405708644180763"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["model = BigramLanguageModel(vocab_size)\n","logits, loss = model(xb, yb)\n","print(f\"{logits=}, {loss=}\")"],"metadata":{"id":"4m1dE7GHb3Oj","executionInfo":{"status":"ok","timestamp":1718176455112,"user_tz":-480,"elapsed":349,"user":{"displayName":"Palash","userId":"17952405708644180763"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2c254582-5b3d-4ce7-b10b-fdc0a6be8aa0"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["logits=tensor([[ 0.0926,  0.5827, -0.6639,  ...,  0.8576,  0.5767, -0.1432],\n","        [-0.0906, -1.1605,  1.2265,  ..., -0.3734, -0.5206,  0.7613],\n","        [-0.8223, -0.5409,  0.1870,  ..., -1.3379,  0.0693,  2.2249],\n","        ...,\n","        [-0.2609,  0.7606, -0.9424,  ..., -1.5740,  0.8685, -0.8768],\n","        [ 0.6698,  0.2120,  0.5794,  ...,  0.8288,  2.3338,  0.9831],\n","        [-0.7894, -1.1862,  0.3646,  ..., -1.4089,  1.4106, -0.2585]],\n","       grad_fn=<ViewBackward0>), loss=tensor(4.6343, grad_fn=<NllLossBackward0>)\n"]}]},{"cell_type":"code","source":["# we are starting with a batch and 1 context length to start with\n","\n","def generate_with_no_context(max_new_tokens=20):\n","  #model.generate(torch.tensor([stoi[' ']]*block_size).view(1, -1), max_new_tokens=10)\n","  inp = torch.zeros((1,1), dtype=torch.long) # zero is newline char\n","  out = model.generate(inp, max_new_tokens=max_new_tokens)\n","  print(decode(out[0].tolist())) # 0-index to pick batch index\n","\n","generate_with_no_context()"],"metadata":{"id":"0CqI3oowKz0F","executionInfo":{"status":"ok","timestamp":1718176782189,"user_tz":-480,"elapsed":412,"user":{"displayName":"Palash","userId":"17952405708644180763"}}},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":["## lets try to create and use an optimizer to train model\n","* read API and docs here - https://pytorch.org/docs/stable/optim.html\n","* **ToDo** - learn about different optimizers, esp Adam(W)"],"metadata":{"id":"BlPu6vwVzKR5"}},{"cell_type":"code","source":["optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"],"metadata":{"id":"sYgC1rgHNU19","executionInfo":{"status":"ok","timestamp":1718176509871,"user_tz":-480,"elapsed":2,"user":{"displayName":"Palash","userId":"17952405708644180763"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["batch_size = 32\n","max_steps = 100000\n","for i in range(max_steps):\n","  xb, yb = get_batch('train')\n","  logits, loss = model(xb, yb)\n","  optimizer.zero_grad(set_to_none=True) # this set_to_none is for performance optimization. but can lead to error if try to access grad\n","  loss.backward()\n","  optimizer.step()\n","  if i%10000==0:\n","    print(loss.item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-wct_1L60jVg","executionInfo":{"status":"ok","timestamp":1718176753425,"user_tz":-480,"elapsed":241878,"user":{"displayName":"Palash","userId":"17952405708644180763"}},"outputId":"b439d63a-f38b-40ea-85fb-5a0663a1cfbd"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["4.727841854095459\n","2.5576331615448\n","2.4996368885040283\n","2.397784471511841\n","2.4928104877471924\n","2.463300943374634\n","2.507370948791504\n","2.4625484943389893\n","2.5009632110595703\n","2.4473514556884766\n"]}]},{"cell_type":"code","source":["generate_with_no_context(max_new_tokens=200)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yllT9DxVL-WM","executionInfo":{"status":"ok","timestamp":1718176797861,"user_tz":-480,"elapsed":5,"user":{"displayName":"Palash","userId":"17952405708644180763"}},"outputId":"ffc80d75-689c-41a0-daa0-1265dd048afd"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Wham'de l:\n","MNUTLAs?\n","PA k My the thofim,\n","ARKis, her stoofous w weryon IO:\n","apr ce f ar sor n!\n","So,\n","Towinccan, hen.\n","bu s st LYCLOry aithot: twoforoove nd IO:\n","Youl wngd ha th a ou\n","Co heishimongor we poono\n"]}]},{"cell_type":"markdown","source":["# matrix multiplcation tricks\n","\n","* try examples by hand in notebook"],"metadata":{"id":"_rfTzUvEtXst"}},{"cell_type":"code","source":["# find avg of Cs across all previous Ts(including itself) for each batch(B) separately\n","# hint try pre mat multiple by lower traingle matrix of ones\n","# but if u avg out this prematrix. u can get avg also.\n","# but for (B,T,C) we need pre-mul matrix of size (B,T, T )\n","\n","# |1 0 0|     |A|     |A    |\n","# |1 1 0|  @  |B|  =  |A+B  |\n","# |1 1 1|     |C|     |A+B+C|"],"metadata":{"id":"teLqxOX4TKkp","executionInfo":{"status":"ok","timestamp":1718190254630,"user_tz":-480,"elapsed":406,"user":{"displayName":"Palash","userId":"17952405708644180763"}}},"execution_count":77,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(42)\n","B, T, C = 2,3,4\n","x = torch.randint(0, 10, (B,T,C)).float()\n","x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OYCP9qcXHI21","executionInfo":{"status":"ok","timestamp":1718194816671,"user_tz":-480,"elapsed":494,"user":{"displayName":"Palash","userId":"17952405708644180763"}},"outputId":"021e8f26-f614-4839-e293-bf94f67325a1"},"execution_count":102,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[2., 7., 6., 4.],\n","         [6., 5., 0., 4.],\n","         [0., 3., 8., 4.]],\n","\n","        [[0., 4., 1., 2.],\n","         [5., 5., 7., 6.],\n","         [9., 6., 3., 1.]]])"]},"metadata":{},"execution_count":102}]},{"cell_type":"code","source":["wei = torch.tril(torch.ones((B,T,T)))\n","# because of how torch only cares about merging dimensions - last of first and first of last. So B is not needed, it will just be broadcasted\n","wei = torch.tril(torch.ones((T,T)))\n","print(wei)\n","wei = wei / torch.sum(wei, dim=-1, keepdims=True)\n","print(wei)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bVoZTmcO5_X-","executionInfo":{"status":"ok","timestamp":1718190929069,"user_tz":-480,"elapsed":481,"user":{"displayName":"Palash","userId":"17952405708644180763"}},"outputId":"6d8c8160-20c1-467e-8ae2-2869f958f54a"},"execution_count":84,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 0., 0.],\n","        [1., 1., 0.],\n","        [1., 1., 1.]])\n","tensor([[1.0000, 0.0000, 0.0000],\n","        [0.5000, 0.5000, 0.0000],\n","        [0.3333, 0.3333, 0.3333]])\n"]}]},{"cell_type":"code","source":["wei @ x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4ky9o9Vj8qwJ","executionInfo":{"status":"ok","timestamp":1718190442054,"user_tz":-480,"elapsed":385,"user":{"displayName":"Palash","userId":"17952405708644180763"}},"outputId":"816e1dce-0553-47ec-b6bd-7812f5caf763"},"execution_count":83,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[2.0000, 7.0000, 6.0000, 4.0000],\n","         [4.0000, 6.0000, 3.0000, 4.0000],\n","         [2.6667, 5.0000, 4.6667, 4.0000]],\n","\n","        [[0.0000, 4.0000, 1.0000, 2.0000],\n","         [2.5000, 4.5000, 4.0000, 4.0000],\n","         [4.6667, 5.0000, 3.6667, 3.0000]]])"]},"metadata":{},"execution_count":83}]},{"cell_type":"code","source":["# in summary\n","wei = torch.tril(torch.ones((T,T)))\n","wei = wei / torch.sum(wei, dim=-1, keepdims=True)\n","way1 = wei @ x\n","way1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OamzTSi9_p-S","executionInfo":{"status":"ok","timestamp":1718190969417,"user_tz":-480,"elapsed":565,"user":{"displayName":"Palash","userId":"17952405708644180763"}},"outputId":"0e6d0a90-b6ff-442e-d499-2e876054af37"},"execution_count":86,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[2.0000, 7.0000, 6.0000, 4.0000],\n","         [4.0000, 6.0000, 3.0000, 4.0000],\n","         [2.6667, 5.0000, 4.6667, 4.0000]],\n","\n","        [[0.0000, 4.0000, 1.0000, 2.0000],\n","         [2.5000, 4.5000, 4.0000, 4.0000],\n","         [4.6667, 5.0000, 3.6667, 3.0000]]])"]},"metadata":{},"execution_count":86}]},{"cell_type":"markdown","source":["* another intersting way\n","* try to walk through manually urself - why and how it works."],"metadata":{"id":"-Xned4XaKaus"}},{"cell_type":"code","source":["# another way using softmax on -inf\n","tril = torch.tril(torch.ones((T,T)))\n","wei = torch.zeros((T,T)) # this will be used in self-attention, but this wei wont be just zero but the interaction between k & q\n","wei = wei.masked_fill(tril==0, value=float('-inf'))\n","wei = torch.softmax(wei, dim=-1)\n","print(wei)\n","way2 = wei @ x\n","print(way2)\n","torch.equal(way1, way2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DiijfOn8KVs6","executionInfo":{"status":"ok","timestamp":1718191490683,"user_tz":-480,"elapsed":445,"user":{"displayName":"Palash","userId":"17952405708644180763"}},"outputId":"213b3ea8-af27-493e-826d-7c7177c7c0d0"},"execution_count":100,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1.0000, 0.0000, 0.0000],\n","        [0.5000, 0.5000, 0.0000],\n","        [0.3333, 0.3333, 0.3333]])\n","tensor([[[2.0000, 7.0000, 6.0000, 4.0000],\n","         [4.0000, 6.0000, 3.0000, 4.0000],\n","         [2.6667, 5.0000, 4.6667, 4.0000]],\n","\n","        [[0.0000, 4.0000, 1.0000, 2.0000],\n","         [2.5000, 4.5000, 4.0000, 4.0000],\n","         [4.6667, 5.0000, 3.6667, 3.0000]]])\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":100}]},{"cell_type":"markdown","source":["# let's head on to self-attention"],"metadata":{"id":"7mmPX_36YfcP"}},{"cell_type":"code","source":["torch.manual_seed(1337)\n","B, T, C = 4,8,32\n","x = torch.randn((B,T,C)).float()"],"metadata":{"id":"alfrkVZPLO2L","executionInfo":{"status":"ok","timestamp":1718199490807,"user_tz":-480,"elapsed":336,"user":{"displayName":"Palash","userId":"17952405708644180763"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# single head attention\n","head_size = 16\n","key = nn.Linear(C, head_size, bias=False)\n","query = nn.Linear(C, head_size, bias=False)\n","\n","k = key(x)    # (B,T,head_size)\n","q = query(x)  # (B,T,head_size)\n","\n","wei = q @ k.transpose(-2, -1) * head_size**-0.5# (B,T,h)@(B,h,T) --> (B,T,T) | now, this `wei` is interaction matrix not just zero or uniform\n","# scaling is to control variance of `wei` before feeding to softmax. Otherwise if this scaling is not done, weights will be very high and softmax will cnvert focus to very small number of other tokens\n","# now, we can do the same ops - masking, softmax & multiplication (but to v similar to  k & q)\n","\n","tril = torch.tril(torch.ones((T,T)))\n","wei = wei.masked_fill(tril==0, value=float('-inf')) # this masking is what makes this as decoder. Otherwise, encoder can look upto all chars in given input\n","                                                    # also, difference in decoder is it has another sublayer which takers encoder's input in 2 of k,q,v (which ones?). Also, only in first layer or all layers? ToDo - figure out\n","wei = torch.softmax(wei, dim=-1)\n","\n","value = nn.Linear(C, head_size, bias=False)\n","v = value(x)\n","\n","out = wei @ v # (B,T,T) @ (B,T,h)\n","print(out.shape) # (B, T, h) --> one head output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tluMytd9VeKq","executionInfo":{"status":"ok","timestamp":1718200685139,"user_tz":-480,"elapsed":6,"user":{"displayName":"Palash","userId":"17952405708644180763"}},"outputId":"13bd7228-439e-41dd-ae83-271d26ab71ca"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([4, 8, 16])\n"]}]},{"cell_type":"code","source":["wei[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JFvEpDHBr7D2","executionInfo":{"status":"ok","timestamp":1718200687155,"user_tz":-480,"elapsed":343,"user":{"displayName":"Palash","userId":"17952405708644180763"}},"outputId":"282f9da2-40c7-4a3b-c2e8-bdd0f761de06"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.2853, 0.7147, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.2858, 0.3704, 0.3437, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.2679, 0.3740, 0.2292, 0.1289, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.1926, 0.1797, 0.1312, 0.1444, 0.3521, 0.0000, 0.0000, 0.0000],\n","        [0.1533, 0.1322, 0.2156, 0.2225, 0.0987, 0.1777, 0.0000, 0.0000],\n","        [0.0863, 0.1575, 0.1328, 0.1596, 0.1788, 0.1199, 0.1652, 0.0000],\n","        [0.1044, 0.1764, 0.1101, 0.0950, 0.1406, 0.1058, 0.1436, 0.1241]],\n","       grad_fn=<SelectBackward0>)"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":[],"metadata":{"id":"CQcYeBHCzDQh"},"execution_count":null,"outputs":[]}]}